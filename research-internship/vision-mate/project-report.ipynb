{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VisionMate: Spatial Awareness for the Visually Impaired**\n",
    "\n",
    "> **A real-time assistive technology system that fuses Monocular Depth Estimation with Object Detection to provide audio-visual proximity alerts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìå 1. Project Overview**\n",
    "Traditional Object Detection models (like YOLO) tell us *what* an object is but fail to answer *where* it is in 3D space. For a visually impaired person, knowing \"there is a car\" is useless without knowing if it is **50 meters away (Safe)** or **2 meters away (Danger)**.\n",
    "\n",
    "**VisionMate** solves this by combining state-of-the-art Computer Vision models to create a \"Smart Blind Spot Monitor\" that runs on standard camera hardware (no expensive LiDAR required).\n",
    "\n",
    "#### **Key Features**\n",
    "* **Multi-Modal AI Pipeline:** Fuses **YOLOv8** (Detection) + **Depth Anything V2** (Depth).\n",
    "* **Real-Time Proximity Logic:** Calculates relative distance and defines dynamic \"Safe Zones.\"\n",
    "* **Audio-Visual Feedback:**\n",
    "    * üü¢ **Green:** Safe Zone (> 2.5m)\n",
    "    * üü† **Orange:** Warning Zone (1.5m - 2.5m)\n",
    "    * üî¥ **Red + Audio Alarm:** Danger Zone (< 1.5m) -> **Triggers \"BEEP\" Sound** üîä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üõ†Ô∏è 2. System Architecture**\n",
    "\n",
    "The system processes video feeds in a 4-step pipeline:\n",
    "\n",
    "1.  **Input:** Standard RGB Video Feed.\n",
    "2.  **Parallel Inference:**\n",
    "    * **Branch A:** YOLOv8n detects objects (`Person`, `Car`, `Bus`, `Truck`).\n",
    "    * **Branch B:** Depth Anything V2 generates a pixel-perfect relative depth map.\n",
    "3.  **Sensor Fusion (The Logic Layer):**\n",
    "    * The system extracts the Depth Map region corresponding to the detected Object Bounding Box.\n",
    "    * **Noise Filtering:** Applies a **70th Percentile Filter** to ignore background noise (e.g., the street behind a person).\n",
    "    * **Distance Calibration:** Converts relative depth score ($S$) to meters ($D$) using the inverse formula:\n",
    "        $$D \\approx \\frac{K}{S}$$\n",
    "        *(Where $K$ is a calibrated focal constant, set to 250 for this prototype)*.\n",
    "4.  **Output:** Overlays bounding boxes, distance labels, and injects audio alerts into the video stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üíª 3. Tech Stack**\n",
    "* **Language:** Python 3.10\n",
    "* **Vision Models:** `Ultralytics YOLOv8`, `HuggingFace Transformers (Depth Anything V2)`\n",
    "* **Video Processing:** `OpenCV`, `MoviePy` (for Audio Mixing)\n",
    "* **Hardware Acceleration:** CUDA (NVIDIA T4 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üöÄ 4. Installation & Usage**\n",
    "\n",
    "#### **Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics transformers torch torchvision opencv-python moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üî¨ 5. Methodology & Calibration**\n",
    "One of the key challenges in monocular depth estimation is that the output is \"unitless\" (relative intensity 0-255). To solve this, we implemented a **Calibration Protocol**:\n",
    "\n",
    "1.  We measured a reference object (Person) at a known distance (approx. 2 meters).\n",
    "2.  We derived a **Calibration Constant (K = 250)** that maps the model's intensity score to real-world meters.\n",
    "3.  **Result:** The system now accurately flags a person at ~1.5 meters as a **\"STOP\"** threat, distinguishing them from a person at 4 meters (Safe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîÆ 6. Future Scope**\n",
    "* **Semantic Voice Navigation:** Upgrade from simple \"Beeps\" to Text-to-Speech (e.g., *\"Car approaching on your left\"*).\n",
    "* **Haptic Integration:** Connect to a vibrating wristband for silent alerts in noisy traffic.\n",
    "* **Auto-Calibration:** Use the average height of a detected human to dynamically calculate the camera's focal length, making the system plug-and-play for any camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Name:** Mohammed Asadullah Shareef  \n",
    "**Hub ID:** HUB2505058"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
