{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 02 Notes: Performance Metrics & Theory\n",
    "\n",
    "**Objective:** Understand how to evaluate an Object Detection model using standard industry metrics (Precision, Recall, F1) rather than just \"Accuracy\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Theory: The Fishing Analogy\n",
    "To understand metrics, imagine you are a fisherman trying to catch **only Red Fish**.\n",
    "\n",
    "##### **A. Precision (Quality)**\n",
    "* **Definition:** Of all the things you *caught* in your net, what percentage were actually Red Fish?\n",
    "* **The \"Oops\" Factor:** If you catch 100 things, and 10 are old boots (False Positives), your precision is 90%.\n",
    "* **High Precision means:** You rarely make False Alarms.\n",
    "* **Formula:** \n",
    "    $$Precision = \\frac{True Positives}{True Positives + False Positives}$$\n",
    "\n",
    "##### **B. Recall (Quantity)**\n",
    "* **Definition:** Of all the Red Fish *in the ocean*, what percentage did you manage to catch?\n",
    "* **The \"Missed\" Factor:** If there are 1,000 fish and you only catch 90, your Recall is terrible (9%), even if your Precision was high.\n",
    "* **High Recall means:** You rarely miss an object.\n",
    "* **Formula:** \n",
    "    $$Recall = \\frac{True Positives}{True Positives + False Negatives}$$\n",
    "\n",
    "##### **C. F1 Score (Balance)**\n",
    "* **Definition:** The harmonic mean of Precision and Recall.\n",
    "* **Why use it?** It penalizes the model if *either* Precision or Recall is bad. It ensures a balanced model.\n",
    "* **Formula:** \n",
    "    $$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Code: Running Validation\n",
    "We used the YOLOv8 Nano model (`yolov8n.pt`) and validated it on the `coco8` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the Model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# 2. Run Validation\n",
    "# data='coco8.yaml' tells YOLO which dataset to test on\n",
    "results = model.val(data='coco8.yaml')\n",
    "\n",
    "# 3. Print Key Metrics\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"mAP@50: {results.box.map50:.4f} (Mean Average Precision at 50% IoU)\")\n",
    "print(f\"Mean Precision: {results.box.mp:.4f}\")\n",
    "print(f\"Mean Recall: {results.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpreting the Plots\n",
    "When you run validation, YOLO saves several images in `runs/detect/val`.\n",
    "\n",
    "##### **A. Confusion Matrix (`confusion_matrix.png`)**\n",
    "This grid shows where the model gets confused.\n",
    "* **Diagonal Line:** These are correct predictions (True Positives).\n",
    "* **Background Row:** The model saw nothing, but predicted something (Ghost detection/False Positive).\n",
    "* **Background Column:** The object was there, but the model missed it (False Negative).\n",
    "\n",
    "##### **B. F1 Curve (`BoxF1_curve.png`)**\n",
    "This chart shows how the F1 score changes as we change the **Confidence Threshold**.\n",
    "* **Confidence Threshold:** The certainty level (0-100%) required for the model to say \"I found something.\"\n",
    "* **The Peak:** The highest point on the blue line represents the best threshold to use for this model to balance Precision and Recall.\n",
    "\n",
    "##### **C. Predictions (`val_batch0_pred.jpg`)**\n",
    "* We compared this against the labels.\n",
    "* **Example:** In our run, the model correctly identified an elephant and a person, but it mistook a black bag for a suitcase (False Positive), which slightly lowered our Precision score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
